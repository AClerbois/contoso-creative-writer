{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook allows you to run Python code interactively.\n",
    "\n",
    "## Getting Started\n",
    "1. Click the Select Kernel button at the top right.\n",
    "2. Choose Python environments and select Python 3.11.9.\n",
    "\n",
    "Run each section of the notebook by clicking the play button on the left side of the code cells.\n",
    "\n",
    "## Learning Outcomes\n",
    "We will focus on four key outcomes:\n",
    "\n",
    "1. Understanding agents and prompt engineering with Prompty.\n",
    "2. Utilizing Prompty tracing for debugging and observability.\n",
    "3. Building and running Contoso Creative Writer.\n",
    "4. Setting up automated evaluations with GitHub Actions.\n",
    "Let’s start with the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Agents and Prompt Engineering with Prompty\n",
    "### What are AI Agents?\n",
    "Contoso Creative Writer is an Agentic Application. In AI, an agent is a program designed to:\n",
    "\n",
    "- Perceive its environment\n",
    "- Make decisions\n",
    "- Take actions to achieve specific goals\n",
    "\n",
    "For Contoso Creative Writer, the goal is to help the marketing team at Contoso Outdoors write well-researched, product-specific articles. This is achieved by four agents, each handling a sub-goal.\n",
    "\n",
    "Contoso Creative Writer is made up of 4 agents: \n",
    "\n",
    "<img src=“../images/agents.png” alt=“Agents in Contoso Creative Writer” width=“900” height=“380”>\n",
    "\n",
    "### Building an Agent with Prompty and Azure OpenAI\n",
    "We’ll focus on the Researcher Agent, which searches for relevant information online using Bing Search and Azure OpenAI with the GPT-3.5 Turbo model.\n",
    "\n",
    "To see it in action, click the play button below. Try changing the instructions to research a topic you’re interested in, for example, “Can you find the best educational material for learning Python programming?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../src/api/agents/researcher'))\n",
    "\n",
    "from researcher import research\n",
    "\n",
    "instructions = \"Can you find the best educational material for learning Python programming.\"\n",
    "\n",
    "research(instructions=instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the Researcher agent work? \n",
    "\n",
    "1. Navigate to the Folder: Click the file icon and go to `src/api/agents/researcher`.\n",
    "\n",
    "2. Files Overview:\n",
    "\n",
    "The researcher folder contains the following 3 files (click on each file to explore it as you read through this list):\n",
    "\n",
    "- `functions.json`: Contains descriptions of tools (find_information, find_entities, find_news) the LLM can use. Each function includes a description of the parameters the LLM needs to generate as input, in this case search queries for the Bing Search API.\n",
    "\n",
    "- `researcher.py`: Implements the functions described in functions.json and includes an execute function to handle instructions and outputs. The execute function passes user input to the researcher.prompty file.\n",
    "\n",
    "- `researcher.prompty`: Standardizes prompts and their execution. It includes the prompt description, model details, and tool parameters from functions.json. The file also contains the base prompt sent to the LLM, including user instructions. \n",
    "\n",
    "\n",
    "#### An introduction to prompt engineering \n",
    "\n",
    "Prompt engineering is the process of designing and refining input prompts to guide generative AI models. With Prompty we can easily test and iterate by changing the user instructions or the base prompt to get better results from the LLM. \n",
    "\n",
    "To illustrate this let's change the instructions we send to the LLM to guide it to use a specific function. In our earlier example you likely had your results returned from the **find_information** function which returns its results in the `web` category, but you'll notice the `entities` and `news` categories are empty since their associated functions weren't called. The instructions we use can change this! \n",
    "\n",
    "1. Find entities: \n",
    "\n",
    "this function is used to find information about people or places. Let's ask the LLM to find information about Guido van Rossum the creator of Python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Can you do some research about the person Guido van Rossum?\"\n",
    "\n",
    "research(instructions=instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find news: \n",
    "\n",
    "This function is used to find news. Let's ask the LLM to find the latest news about Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Can you find the latest news about Microsoft? \"\n",
    "\n",
    "research(instructions=instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to make sure the rest of the workshop works as expected we won't make any changes to the base prompt in `researcher.prompty`, but feel free to make edits it later to see how the results change! \n",
    "\n",
    "Let's now move onto the next Learing outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilizing Prompty tracing for debugging and observabilty\n",
    "\n",
    "When running Applications driven by LLMs, sometimes things don't go as expected! It's important to have a way to debug your LLM workflow so you can see where things are working. \n",
    "\n",
    "Tracing helps you visualize the execution of your prompts and clearly see what inputs are being passed to the LLM. \n",
    "\n",
    "We'll use tracing to also get a better understanding of what's happening in our workflow by calling the functions that run the researcher, product and writer agents. \n",
    "\n",
    "Do the following to initiate tracing: \n",
    "- Run the cell below and then click the generated link (this might take a few minutes to appear)\n",
    "- Once in the new tab click on the name `openai_chat` in the first row\n",
    "- You should now be in the `converstions` tab. This tab contains the full prompt that will be sent to the LLM in Markdown format for readability. \n",
    "- Scroll to the top of the output and particularly look at the Research and Product Information sections. These sections contain the results generated by the Researcher and Product agents. If either of them were not producing the expected results we could easily spot it here! \n",
    "- Click on the `Raw Json` tab. This shows exactly what is sent to the LLM in the format it accepts, which is Json. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../src/api'))\n",
    "\n",
    "from promptflow.tracing import start_trace\n",
    "from agents.researcher import researcher\n",
    "from agents.product import product\n",
    "from agents.writer import writer\n",
    "\n",
    "research_context = \"Can you find the latest camping trends and what folks are doing in the winter?\"\n",
    "product_context = \"Can you use a selection of tents and sleeping bags as context?\"\n",
    "assignment_context = '''Write a fun and engaging article that includes the research and product information. \n",
    "    The article should be between 800 and 1000 words.\n",
    "    Make sure to cite sources in the article as you mention the research not at the end.'''\n",
    "\n",
    "research_result = researcher.research(research_context)\n",
    "product_result = product.find_products(product_context)\n",
    "\n",
    "start_trace()\n",
    "\n",
    "writer_result = writer.write(\n",
    "        research_context,\n",
    "        research_result,\n",
    "        product_context,\n",
    "        product_result,\n",
    "        assignment_context,\n",
    "    )\n",
    "\n",
    "processed_writer_result = writer.process(writer_result)\n",
    "print(processed_writer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing is useful for deugging and observability both locally and in production. \n",
    "\n",
    "Let's now move on to the next section, building the Contoso Creative Writer Application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and running Contoso Creative Writer \n",
    "\n",
    "Now that we understand how the application works it's time to build it. \n",
    "\n",
    "To complete these next two learning outcomes you'll need to use the terminal. \n",
    "\n",
    "If it’s not already visible, you can open it by clicking on the hamburger menu at the top left of the page, clicking Terminal and then selecting New Terminal.\n",
    "\n",
    "Once your terminal is open, copy and past the following commands in the terminal and press enter after each one to run it. \n",
    "\n",
    "1. Starting the FastAPI server \n",
    "\n",
    "    Navigate to the src/api folder  with the following command \n",
    "\n",
    "    ```shell\n",
    "    cd ./src/api\n",
    "    ```\n",
    "\n",
    "    Run the FastAPI webserver with the following command \n",
    "\n",
    "    ```shell\n",
    "    fastapi dev main.py\n",
    "    ```\n",
    "\n",
    "Do not click open browser if prompted. \n",
    "\n",
    "Next you'll need to change the visibility of the API's 8000 and 5173 ports to public in the `PORTS` tab. You can do this by right clicking on the visibility section of the port, selecting port visibility and setting it to public. The ports tab should look like this:\n",
    "\n",
    "<img src=\"../../images/ports.png\" alt=\"Screenshot showing setting port-visibility\" width=\"800px\" />\n",
    "\n",
    "\n",
    "2. Running the web app \n",
    "\n",
    "    Once you've completed the above steps. You'll need to open a **new terminal** and navigate to the web folder. you can open a new terminal by clicking on the hamburger menu at the top left of the page, clicking Terminal and then selecting New Terminal. \n",
    "\n",
    "    In the terminal run the following commands \n",
    "\n",
    "    ```shell\n",
    "    cd ./src/web\n",
    "    ```\n",
    "    \n",
    "    First install node packages:\n",
    "\n",
    "    ```shell\n",
    "    npm install\n",
    "    ```\n",
    "\n",
    "    Then run the web app with a local dev web server:\n",
    "    \n",
    "    ```shell\n",
    "    npm run dev\n",
    "    ```\n",
    "\n",
    "Once you've run the above command you should see an `http://localhost:5173/` link in the terminal. Click this link or click the `open on browser` button that comes up as a Gitub notification in the bottom right corner of the screen. Select the `continue` button and that's it! \n",
    "\n",
    "You can now test out the app by clicking `Example` to fill out the example information and then clicking `Start Work` to get Contoso Creative Writer to generate an article. \n",
    "\n",
    "You can also see which agent steps are being carried out in what order by click on the small bug button at the bottom right of the Application. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up automated evaluations with Github Actions \n",
    "\n",
    "Contoso Creative Writer is set up to run a CI/CD pipeline, which stands for Continuous Integration and Continuous Deployment. This is a series of automated steps that streamline the process of delivering software.\n",
    "\n",
    "- Continuous Integration (CI): This involves automatically integrating code changes from multiple contributors into a shared repository several times a day.\n",
    "- Continuous Deployment (CD): This involves automatically deploying the integrated code to production environments. \n",
    "\n",
    "In this sample code the CI/CD pipeline includes the following: \n",
    "1. Build and Deploy: Automatically building and deploying the latest version of the code to production (This helps us confirm things are working as expected.)\n",
    "2. Evaulations: Automatically sending example research, product and assignment instructions to Contoso Creative Writer and running evaulations on the produced article to see how fluent, grounded, relevant and x the final response was given the questions. \n",
    "\n",
    "\n",
    "To set up CI/CD with GitHub actions on your repository, open a new terminal and run the following command:\n",
    "\n",
    "```shell\n",
    "azd pipeline config\n",
    "```\n",
    "\n",
    "To see the results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
