{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ú®This Jupyter notebook allows you to run Python code interactively üêç‚ú®\n",
    "\n",
    "## Getting Started\n",
    "1. Click the **Select Kernel** button at the top right.\n",
    "2. Choose Python environments and select **Python 3.11.9**.\n",
    "\n",
    "Run each section of the notebook by clicking the play button on the left side of the code cells.\n",
    "\n",
    "## Learning Outcomes\n",
    "We will focus on four key outcomes:\n",
    "\n",
    "1. Understanding agents and prompt engineering with Prompty.\n",
    "2. Utilizing Prompty tracing for debugging and observability.\n",
    "3. Building and running Contoso Creative Writer.\n",
    "4. Setting up automated evaluations with GitHub Actions.\n",
    "\n",
    "Let‚Äôs start with the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Agents and Prompt Engineering with Prompty\n",
    "### 1.1. What are AI agents?\n",
    "Contoso Creative Writer is an Agentic Application. \n",
    "\n",
    "In artificial intelligence an agent is a program designed to:\n",
    "\n",
    "- Perceive its environment\n",
    "- Make decisions\n",
    "- Take actions to achieve specific goals\n",
    "\n",
    "For Contoso Creative Writer, the goal is to help the marketing team at Contoso Outdoors write well-researched, product-specific articles. \n",
    "\n",
    "Contoso Creative Writer is made up of 4 agents that help achieve this goal.\n",
    "<br>In the file explorer to the left open and explore the **src/api/agents** folder. This folder contains 4 sub-folders, one for each agent.  \n",
    "\n",
    "<img src=\"../../images/agents.png\" alt=\"Agents in Contoso Creative Writer\" width=\"900\" height=\"380\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. How is an AI agent built?\n",
    "\n",
    "Each agent in Contoso Creative Writer is built with [Prompty](https://prompty.ai/)! \n",
    "\n",
    "#### 1.2.1 What is Prompty?\n",
    "Prompty is a new asset class and file format for LLM prompts that aims to provide observability, understandability, and portability for developers.\n",
    "\n",
    "The tooling for Prompty comes together in three ways:\n",
    "\n",
    "**i. The prompty file asset:**\n",
    "- A Prompty file is an asset class not tied to any language as it uses markdown format with YAML to specify your metadata. \n",
    "- A Prompty file has two main parts:\n",
    "\n",
    "    - **Front Mattter:** This is the first section of the prompty file that is written in YAML and inside the two `---`'s. \n",
    "    <br>It includes basic details about the prompt, the model configuration and any other relevant prompty inputs. \n",
    "\n",
    "    - **Prompt Template:** This is the base prompt that is sent to the LLM once the prompty is executed. Any values either in the\n",
    "    <br>*sample* section of the front matter or directly provided as input variables to the prompty are dynamically added to the prompt\n",
    "    <br>using the Jinja formatter. For example if a user inputs their name as 'Marlene', the {{name}} variable will be replaced with the \n",
    "    <br>appropriate value.\n",
    "\n",
    "\n",
    "**ii. The VS Code extension tool:**\n",
    "- The Prompty extension allows you to run Prompty files directly in VS Code. \n",
    "- It provides an interface for viewing traces and other information about your prompt\n",
    "- It has been pre-installed for this workshop, but you can also find it in the Visual Studio Code Marketplace.\n",
    "\n",
    "**iii. Runtimes in multiple programming languages:**\n",
    "To execute a Prompty file asset in code, you can use one of the supporting runtimes such as:\n",
    "- Prompty Core (python) - The python Prompty core package is the base package needed to run a prompty asset file.\n",
    "- Promptyflow (python) - Use Prompty with Promptflow as your runtime and logic orchestrator.\n",
    "- LangChain (python) - Use the experimental LangChain Prompty runtime.\n",
    "- Semantic Kernal (csharp) - Use the experimental Semantic Kernal Prompty runtime.\n",
    "\n",
    "### 1.3. Building an AI Agent\n",
    "\n",
    "To help us understand practically how we build an AI agent will build the **Researcher Agent** step by step.\n",
    "<br>In order to build the Researcher agent you will complete the following 3 steps:\n",
    "\n",
    "#### Steps to build the researcher agent\n",
    "- [ ] Step 1: Build a multi-lingual query generator\n",
    "- [ ] Step 2: Give the query generator a list of functions available\n",
    "- [ ] Step 3: Build the tools and execute the research\n",
    "\n",
    "##### **Step 1:** Build a multi-lingual query generator\n",
    "- At it's core the researcher agent generates queries that can be used to look for information online using Bing Search. \n",
    "- We also want it to be able to find search results in multiple languages. This can be done using a single prompty file. \n",
    "- To see this in action open the [researcher-0.prompty](researcher/researcher-0.prompty) file and click the run button on the top right of the file. \n",
    "\n",
    "‚úÖ To complete this step update the **sample** category in the front matter to change the instructions and include a market code to in the instructions to change the language. \n",
    "\n",
    "\n",
    "##### **Step 2:** Give the query generator a list of functions available\n",
    "- In order for the researcher to generate even better queries it needs to know which search functions are avaialble to it. \n",
    "- Prompty allows us to pass information to the LLM in the form of a json file using the **${file:filename.json}** format, where **filename** is the name of an existing json file. \n",
    "- In this step we want to give the LLM a list of the functions (sometimes called tools) that it can choose from.\n",
    "- The LLM should examine the user instructions and return the name of the most appropriate function to use and the query that will be passed to that function as a parameter. \n",
    "- Examine the [functions.json](researcher/functions.json) file. You should see the **find_information**, **find_entities** and **find_news** functions. Read the descriptions to understand what each function does! \n",
    "- Open the [researcher-1.prompty](researcher/researcher-1.prompty) file and note that **${file:functions.json}** has been added to tools under the parameters section in the front matter. This is all that needs to be done to pass it to the LLM using prompty. \n",
    "- Click the run button on the top right of the file to see all of this in action. \n",
    "\n",
    "‚úÖ To complete this step update the instructions to influence which function the LLM chooses. For example to get the find_news function chosen use instructions: 'Can you find the latest news about Microsoft?'\n",
    "\n",
    "##### **Step 3:** Build the functions and execute the research\n",
    "- The researcher has now selected which function to use and has generated a query and possibly a market code to pass to it. \n",
    "- We now need to write the Python code for these functions that will pass the queries to the Bing Search API. \n",
    "- We will also use a function in Python to execute the prompty file, instead of running it manually in VS Code. \n",
    "- Open the the [researcher3.py](researcher/researcher3.py) file to see this code and try and understand what each function does. \n",
    "- To put everything together click the play button on the left of the jupyter notebook code cell below below.  \n",
    "- The cell calls the **research** function to run the code in researcher3.py.\n",
    "\n",
    "‚úÖ To complete this step run the code in the cell below and test it out with different instructions and languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/researcher'))\n",
    "\n",
    "from researcher3 import research\n",
    "\n",
    "instructions = \"Can you find the best educational material for learning Python programming.\"\n",
    "\n",
    "research(instructions=instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congratulations you've build your first AI Agent with Promptyüéâ\n",
    "- [‚úÖ] Step 1: Build a multi-lingual query generator\n",
    "- [‚úÖ] Step 2: Give the query generator a list of functions available\n",
    "- [‚úÖ] Step 3: Build the tools and execute the research\n",
    "\n",
    "We can now succesfully move on to learning outcome 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilizing Prompty tracing for debugging and observabilty\n",
    "\n",
    "When running Applications driven by LLMs, sometimes things don't go as expected! \n",
    "<br>It's important to have a way to debug your LLM workflow so you can see where things are working. \n",
    "<br>Tracing helps you visualize the execution of your prompts and clearly see what inputs are being passed to the LLM. \n",
    "\n",
    "To illustrate how to use tracing in prompty let's build and debug a custom agent!\n",
    "\n",
    "### 2.1 Run and debug a custom social media agent\n",
    "\n",
    "<img src=\"../../images/socialmediaagent.png\" alt=\"social media agent\" width=\"600\" height=\"350\">\n",
    "\n",
    "You should see a **socialmedia** folder in the workshop folder. This folder contains:\n",
    "\n",
    "- [social.py](socialmedia/social.py) file:** Uses the *execute_social_media_writer_prompty* and *run_social_media_agent* functions to send the inputs and prompts to the LLM. \n",
    "<br>It also imports the research function from the researcher agent to let it access information from online.  \n",
    "\n",
    "- [social.prompty](socialmedia/social.prompty) file:** This contains the base prompt for the social media agent.\n",
    "\n",
    "üêû**BUG ALERT:** I have purposefully left a bug in the prompty file. We will use tracing to quickly spot the bug and fix it! \n",
    "\n",
    "##### Steps to build and debug the social media agent\n",
    "- [ ] Step 1: Run the code for social media agent\n",
    "- [ ] Step 2: Use Prompty Tracing to identify and fix the bug\n",
    "\n",
    "\n",
    "##### **Step 1:** Run the code for social media agent\n",
    "- In order to run the code for social media agent click the play button on the left of the jupyter notebook code cell below. \n",
    "- What do you observe that is strange from the results?  \n",
    "\n",
    "‚úÖ To complete this step run the jupyter notebook cell below and try guess what's causing the bug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to sys.path\n",
    "sys.path.append(os.path.abspath('../../docs/workshop/socialmedia'))\n",
    "\n",
    "from social import run_social_media_agent\n",
    "\n",
    "research_instructions = \"Find information about AI Agents\"\n",
    "social_media_instructions = \"Write a fun and engaging twitter thread about AI Agents given the research.\"\n",
    "\n",
    "run_social_media_agent(instructions=research_instructions, social_media_instructions = social_media_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2:** Use Prompty Tracing to identify and fix the bug\n",
    "- To see the trace generated by Prompty open the **workshop** folder in the file explorer and look for a **.runs** folder in it. \n",
    "- Select this folder and click the **.tracy** file at the top of the folder. \n",
    "- This file shows you all the information that prompty has sent to or recieved from the LLM. \n",
    "- In this specific case look at the Prompt Tokens amount. This amount is not enough for us to generate a thread on twitter. \n",
    "- Go to the [social.prompty](socialmedia/social.prompty) file and edit the *max_tokens* amount to make it 1500!\n",
    "\n",
    "‚úÖ To complete this step rerun the cell above and confirm the full thread is generated \n",
    "<br>(you may beed to restart the notebook to get the changes to take effect by clicking the *restart* button at the top of the notebook)\n",
    "\n",
    "\n",
    "##### Congratulations you've succesfully used Prompty tracing for debuggingüéâ\n",
    "- [‚úÖ] Step 1: Run the code for social media agent\n",
    "- [‚úÖ] Step 2: Use Prompty Tracing to identify and fix the bug\n",
    "\n",
    "Now that we have a good understanding of how to build and debug agents with Prompty let's run Contoso Creative Writer, a multi-agent solution! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and running Contoso Creative Writer \n",
    "\n",
    "**To complete the next two learning outcomes you'll need to use the terminal.** \n",
    "\n",
    "If it‚Äôs not already visible, you can open it by clicking on the hamburger menu at the top left of the page, clicking Terminal and then selecting New Terminal.\n",
    "<br>Once your terminal is open, copy and past the following commands in the terminal and press enter after each one to run it. \n",
    "\n",
    "##### **Step 1:** Starting the FastAPI server \n",
    "\n",
    "1. We'll start by running the Python FastAPI Server\n",
    "\n",
    "    Navigate to the src/api folder  with the following command \n",
    "\n",
    "    ```shell\n",
    "    cd ./src/api\n",
    "    ```\n",
    "\n",
    "    Run the FastAPI webserver with the following command \n",
    "\n",
    "    ```shell\n",
    "    fastapi dev main.py\n",
    "    ```\n",
    "\n",
    "    Do not click open browser if prompted. \n",
    "\n",
    "    Next you'll need to change the visibility of the API's 8000 and 5173 ports to public in the `PORTS` tab. \n",
    "    <br>You can do this by right clicking on the visibility section of the port, selecting port visibility and setting it to public. The ports tab should look like this:\n",
    "\n",
    "    <img src=\"../../images/ports.png\" alt=\"Screenshot showing setting port-visibility\" width=\"800px\" />\n",
    "\n",
    "##### **Step 2:** Starting the Web Server\n",
    "\n",
    "1. Running the web app \n",
    "\n",
    "    Once you've completed the above steps. You'll need to open a **new terminal** and navigate to the web folder. \n",
    "    <br>You can open a new terminal by clicking on the hamburger menu at the top left of the page, clicking Terminal and then selecting New Terminal. \n",
    "\n",
    "    In the terminal run the following commands \n",
    "\n",
    "    ```shell\n",
    "    cd ./src/web\n",
    "    ```\n",
    "    \n",
    "    First install node packages:\n",
    "\n",
    "    ```shell\n",
    "    npm install\n",
    "    ```\n",
    "\n",
    "    Then run the web app with a local dev web server:\n",
    "    \n",
    "    ```shell\n",
    "    npm run dev\n",
    "    ```\n",
    "\n",
    "    Once you've run the above command you should see an **http://localhost:5173/** link in the terminal. \n",
    "    <br>Click this link or click the **open on browser** button that comes up as a Gitub notification in the bottom right corner of the screen. \n",
    "    <br>Select the **continue** button and you should now see the app appear on your screen!\n",
    "\n",
    "You can now test out the app by clicking **Example** to fill out the example information and then clicking **Start Work** to get Contoso Creative Writer to generate an article. \n",
    "<br>You can also see which agent steps are being carried out in what order by click on the small bug button at the bottom right of the Application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up automated evaluations and deployment with Github Actions \n",
    "\n",
    "Contoso Creative Writer is set up to run a CI/CD pipeline, which stands for Continuous Integration and Continuous Deployment. This is a series of automated steps that streamline the process of delivering software.\n",
    "\n",
    "In this sample code the CI/CD pipeline includes the following: \n",
    "1. **Build and Deploy:** Automatically building and deploying the latest version of the code to production (This helps us confirm things are working as expected.)\n",
    "2. **Evaulations:** Automatically sending example research, product and assignment instructions to Contoso Creative Writer and running evaulations on the produced article to see how fluent, grounded, relevant and x the final response was given the questions. \n",
    "\n",
    "\n",
    "To set up CI/CD with GitHub actions on your repository, **open a new terminal** and: \n",
    "\n",
    "1. Run the following command:\n",
    "\n",
    "    ```shell\n",
    "    azd pipeline config\n",
    "    ```\n",
    "\n",
    "    - select an environment name like yourname-aitour\n",
    "    - select the recommended subscription by pressing enter\n",
    "    - select `Canada East` as the Azure Location \n",
    "    - When asked to Log in using the Github CLI type in `Y`\n",
    "    - Choose `HTTPS` as the preferred protocol for Git Operations \n",
    "    - Select `Y` to Authenticate with your Github credentials. \n",
    "    - Choose Login with a web browser to authenticate Github CLI and follow the authentication instructions. \n",
    "    - You may be asked if you want to commit and push your local changes. Choose `n`\n",
    "    - You should see two links in your terminal. Select the Link to view your pipeline status. \n",
    "\n",
    "2. You should now be on a page that shows all the workflows. It should look similar to the image below. \n",
    "\n",
    "- Click on the workflow named evaluate (outlined in red in the image).\n",
    "- It may need a few minutes to complete but once complete you should see the evaluated results on the page. \n",
    "\n",
    "You should see a table with some scores for relevance, fluencey, coherence and groundedness. The scores are from 1-5, with 5 being the highest mark. These are used to help us know how well the model is performing. This is helpful as a metric and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
